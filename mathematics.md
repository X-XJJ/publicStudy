

G=(V,E), 一个匹配 M （M包含于E） 需满足 e1 ∩e2=∅，对于所有e1、e2∈M
最大匹配 |M|最多（|M|是这个匹配中的边的个数）
极大匹配

黄金标准

算法1.2 找一个极大匹配M，，算法1.2是 2-近似 的

（1）用同一个下界，能否得到更好的结果？——不能，算法的极限是2
（2）其他下界，能否更好？——不能，这个问题太难，100年内找不到更好的
（3）换一个算法，能否更好？——

- 最大化问题
A(I) ≥ρOPT(I)，A称为 ρ-近似算法，ρ∈[0,1]


- 动态规划，背包问题。动态规划的本质是遍历（初始化、迭代公式、最优解的选择）
- S(1,p)是解，A(1,p)是值，用值的存储，代替解的存储，省空间和计算量
Pmax = max profix(ai) ≤OPT≤n×Pmax

- 动态规划 一般选控制相对误差的方法。遍历 一般选控制相对误差的方法。

- 近似方案：一群算法中挑一个A，使得 A(I)≥(1-ε)OPT(I)
- 多项式时间近似方案 PTAS，有理论价值，但实际运用的价值不大，数太大算不出来
- EPTAS，有效PTAS
- 全多项式时间近似方案 FPTAS，fully，对很多NP难问题能做的最佳结果
- 拟多项式时间：在一元编码时，是多项式时间。一元编码即一进制编码


- 优化问题 转化为 判定问题，任意优化问题都可以转化为判定问题。
  - eg：背包问题，物品集合 I，找 S≤I，使得 a(S)≤B 且 收益 p(S)最大。
  - 问：是否存在一个子集 S≤I，使得 a(S)≤B，p(S)≥C？C可以视为最优解下界？？


- 判定性问题：回答职能是“是”或“否”。
  - 集合A中是否存在大于5的数。“是”的验证可以在多项式时间内，找一个符合的数出来验证大于5即可。“否”则验证需要将集合中所有数都和5比较一次。
  - a是否为质数。“是”则验证。“否”
  - 是否能将 {a1...,an} 分成两个相等的集合。“是”验证举一个例子即可。“否”则验证需要列出所有可能的分组情况一一验证。

- P问题：“是”和“否”都能在多项式时间验证的 判定性问题
- NP问题：“是”可以在多项式时间验证的 判定性问题，P 包含于 NP
- NP完全问题：NP-complete问题，.SAT问题，判定性问题
- NP难问题：NP-hard问题，比NPC更难


- 多项式归约
- 划分问题 是 NP-hard的。用SAT问题多项式归约而来。假定 划分问题 无 多项式时间最优算法
  - 给定 n 个数 I = {a1, a2,...,an}，问：是否存在一个划分(S1, S2)，使得 S1的∑aj = S2的∑aj = 1/2 ∑aj、j从1累加到n
  - 划分 = 这两个集合不交、交集为空，且 这两个集合的 并集 = 全集 I

- 定理：背包问题是NP-hard
  - 给定划分问题的一个实例 I，构造背包问题的一个实例 I' = {1,2,...,n}，其中，物品 j 的大小为 aj，收益 pj = aj，背包容量 B = 1/2 ∑aj、j从1~n累加
  - 需要证明，划分问题实例 I 有解 <=> I' 的最优值为 B，即 将 背包问题 归约到 划分问题
  - =>：若实例 I 有解 (S1,S2)，则可构造 I' 的一个可行解 S = {j | aj∈S1}，则 a(S) = S1的∑aj = B，p(S) = a(S) = B，即 OPT(I')≥B。对任意可行解 S'，有 p(S') = a(S') ≤B，即 OPT(I') ≤B。所以，OPT(I') = B。
  - <=：若 I'的最优值为 B，令 Sx 表示最优解，S1 = {aj | j∈Sx}，S2 = I\S1。则 S1的∑aj = a(Sx) = p(Sx) = B，S2的∑aj = B。所以，(S1,S2)为I的一个可行解。


- 定理：装箱问题 是NP-hard的
  - 给定划分问题的一个实例 I，构造装箱问题的一个实例 I'，物品 j 的大小 aj' = aj/B，，=(S1,S2)
  - I 有解 <=> I'的最优值为 2


- 确界原理：非空的有上界的集合必有上确界；非空的有下界的集合必有下确界。
  - 上确界：sup(X)，上界中最小的一个，集合X的上确界，supremum，取上限函数
  - 下确界：inf(X)，下界中最大的一个，集合X的下确界，infimum，取下限函数
  - eg：一个集合X可以有很多个上界、下界，(1,4) 的上界可以是4、5、6、7……下界可以是0、-1、-2……
  - eg：集合 M=[1,4]，sup(M)=4，inf(M)=1，max(M)=4，min(M)=1
  - eg：集合 N=(1,4)，sup(N)=4，inf(N)=1，max(N)=无，min(N)=无


# 数学规划问题（数学建模）
- 线性规划：目标函数 & 约束 都是线性的。与 非线性规划 对应。多项式时间可解。
- 整数规划：所有决策变量范围为整数。整数规划 难于 线性规划。
- 混合整数规划：部分决策变量范围为整数。
- 0-1 整数规划：决策变量只能取0和1。

- 如何求整数规划
 - 最优解：动态规划、分支定界……
 - 近似解：LP-rounding、原始对偶……
   - LP-rounding取整：先求松弛形式（线性规划）的最优解（分数解），再取整为整数解（取整，是要求技巧的关键步骤）
   - 原始对偶：通过对偶规划、互补松弛条件来构造可行解。
     - 数学规划的极限：存在“整数间隙”决定了数学规划的极限。
     - 整数间隙 =  最优分数解/最优整数解。这个近似比无法得到，只能叫做“整数间隙”。∵不知道我们构造出的整数解是否为最优解（整数最优解NP-hard求不出来，∴无法确定是不是最优整数解）


- 数学规划，可以重构。一个问题，有很多种刻画方法。也可以将一个问题的规划，转化为另一种规划形式。eg：传统整数规划 转为 出价语言，传统整数规划 转为 半正定矩阵类型，诸如此类。


- 松弛变量：在约束中引入的额外变量，用于表示原始问题中每个不等式约束的“松弛”或“盈余”程度。
  - eg：有约束 a1·x1 + a2·x2 ≤b1，则令 a1·x1 + a2·x2 + x3 = b1，其中，x3就是一个松弛变量。

- 整数规划的【松弛问题】：把原整数规划问题，去掉整数的条件，剩下的目标函数和约束条件构成的线性规划问题，就是此整数规划的松弛问题。
  - eg：x≥3且x为整数，松弛后：x≥3。
  - eg：x∈{0,1}，松弛后：0 ≤x ≤1

[【最优化(一)】最优化问题概述和分类](https://zhuanlan.zhihu.com/p/534485974)

[多目标优化问题](https://www.jianshu.com/p/3d906bb63480)
[数学建模（二）---多目标优化](https://zhuanlan.zhihu.com/p/616882532)
[epsilon-约束方法](https://blog.csdn.net/faithful1/article/details/136448603)
[【多目标规划问题求解】ε-约束算法](https://blog.csdn.net/weixin_44786238/article/details/126068290)

- 博弈的 minmax 问题、maxmin 问题，与 多目标优化问题 的区别
  - minmax 和 maxmin，涉及2类变量 x、y，即 博弈的双方，在一方x的情况确定后，找在此情形下，另一方的情况。
  - 多目标优化，含有多个优化目标，但每个优化目标 可能都只和一类决策变量x相关

[算法中的最优化方法与实现（第11课 多目标优化问题）](https://blog.csdn.net/komjay/article/details/134758386)
[数学中常见的maxmin，minmax](https://blog.csdn.net/caomin1hao/article/details/109265464)
[算法——结合实例了解Minimax算法（极小化极大算法）](https://blog.csdn.net/oopxiajun2011/article/details/145690313)

Reinforce：xx 提出基于强化学习的NUMA感知方法，最小化任务总协同执行运行时间。
provides a reinforcement learning based NUMA-aware approach to minimise the total co-execution runtime of the task。



# 对偶问题
- 原始线性规划（LP）的标准形式：目标 min，约束都是 ≥，决策变量 ≥0
- 对偶线性规划（DLP）目标为 max
- LP的 一行约束条件 对应DLP的 一个对偶变量
- LP的 目标函数的系数 对应DLP的 约束条件的右侧
- LP的 约束条件的右侧 对应DLP的 目标函数的系数


- 简明版转换规则，大同小异 [运筹学——两道例题教你学会写对偶问题(无底噪无废话纯享版)](https://www.bilibili.com/video/BV1Nd4y1V7j2)

- 具体版转换规则，22min左右，比较&图示，直观说明两个问题最优解的关系 [【民科运筹】抽象的对偶问题如何理解——并考试](https://www.bilibili.com/video/BV1Ci4y1e7tf)

- 卖成品 or 卖原料，具体例子理解 [【线性规划】对偶问题的实际意义与重要性质](https://www.bilibili.com/video/BV1sz411B7Zm)

- 卖成品 or 卖原料，12min左右，场景理解“反常”和变换时“符号”的关系 [运筹学-写对偶问题-在自己生产和卖原材料之间灵活切换！](https://www.bilibili.com/video/BV1JE411E734)

- 弱对偶定理：原max的解 ≤对偶min的解。对偶是原min的一个下界。理解：卖成品收益≤卖原料的收益，才会选择卖原料
  - 同理，原min的解 ≥对偶max的解。对偶是原max的一个上界
- 强对偶定理：若原问题&对偶问题都存在可行的最优解，则 OPT(原) = OPT(对偶)。——凸优化问题才一定具有强对偶性。线性规划 = 特殊的凸优化问题。
- 互补松弛性：[运筹学-对偶的性质，高潮区：互补松弛定理](https://www.bilibili.com/video/BV1zE41147Ym)
  - 贯彻的操作：对偶问题存在最优解，将解带入对偶问题的约束；原问题存在最优解，将解代入原问题的约束。
  - 原问题的最优解 与 对偶问题的松弛变量 乘积为0
  - 对偶问题的最优解 与 原问题的松弛变量 乘积为0
  - 按转换对偶问题的从前到后的顺序一一对应相乘，每一对“解×变量”的结果都 = 0
  - 同理，满足互补松弛性的解 是最优解

- 从 最优化理论&拉格朗日函数 角度，转化对偶问题，[11.4 对偶是怎么来的](https://www.bilibili.com/video/BV1WE42157rN)


- 对偶问题：可能减少约束条件的个数 or 决策变量的个数 等等，更容易求解

- 原始对偶算法：通过对偶问题来求解原问题的最优解or近似解的一类方法

- 拉格朗日函数：将具有约束的优化问题转化为无约束优化问题的方法。拉格朗日函数通过引入拉格朗日乘子（Lagrange Multipliers），将原始问题的约束条件整合到目标函数中，从而将原始问题转换为对偶问题。

- 拉格朗日对偶


- 影子价格：[第九课 影子价格](https://www.bilibili.com/video/BV1pA411H76k)
  - 影子价格 = 边际价格or拉格朗日价格。
  - 表示在给定的优化模型中，如果某个资源或约束条件的可用数量增加一个单位，最优目标函数值（例如成本最小化问题中的成本 or 效用最大化问题中的效用）将增加或减少的量。影子价格提供了资源边际价值的度量，即资源的额外单位对目标函数的影响。
  - 对偶问题最优解中的各个解的数字 依次对应的就是原问题中各个资源（即各个约束）的影子价格。
  - 影子价格的经济意义：每增加1单位的某种资源，最终收益增加多少单位
  - 判断资源在达到最优计划时 是否有剩余
    - 影子价格 = 0：该影子价格对应的资源有剩余，理解：没有用上的资源都不值钱。
    - 影子价格＞ 0：该影子价格对应的资源无剩余。

- 用原始对偶 导出 最大流最小割 问题

- 最大流
  - 实例：城市地下水管道，为什么有写路段容易撑爆了淹水。点：管道之间的接口：边：该管道的最大容量
  - 找一个每条边上 流量的 流经方案，找到从起点到终点能传输的最大流量
  - 两个点 i j，点i和点j之间的边容量 c_ij，图 G=(V,A)，点集V，弧集A。起点s，终点t。
  - 标准下，每个点 流入流量 = 流出流量。
    - ∑条件“j:(i,j)∈A”，从i流出到j的流量。表示 找每条 j在变、i固定的弧(i,j)。
    - ∑条件“j:(j,i)∈A”，从j流入到i的流量。表示 找每条 j在变、i固定的弧(i,j)。
    - 但在规划中，= 也可以写成 ≤，因为但凡 ＜ 的情况，都不是可行解。好处是，可变成 LP 标准形式。

- 最小割，即 最小切
  - 实例：战争中，用最小的代价切断补给线
  - 割集：点集 X ∪X' = V，且 X ∩X'=空。则点集X 到 点集X' 之间的弧集合，即为 割集
  - 找一个点集X，使得费用（即容量c_ij）最小，小的好炸，破坏成本也低。
  - 一个割集，对应一个 最大流对偶问题的整数规划的一个可行解。
  - 三次转化：弱对偶理论（最大流的对偶问题），可行域大小（对偶的线性规划问题放缩为整数规划），可行域大小（对偶整数规划是最小割问题的可行解）



# 3blue1brown 的视频


# 概率论
- 样本空间S：随机试验E中所有可能的结果构成的集合。E的每个结果是“样本点”
- 随机事件：样本空间S的子集。简称“事件”。
  - 由一个样本组成的单点集，叫“基本事件”。
  - 当且仅当“这个子集中的一个样本点出现”时，叫“一个事件发生”

- 随机变量：离散型or连续型。同时，根据随机变量的类型不同，其分布也分为 离散型分布、连续型分布。

- 概率：P(A)表征事件A在一次试验中发生的可能性的大小。
  - 等可能概型中，事件A的概率 P(A) = A包含的基本事件数/S中基本事件的总数

- 条件概率：
  - “|…”= 在…的条件下。在“|”出现时，是在讨论总概率空间中的一个有限的部分。
  - 在A发生的条件下 B发生的概率 P(B|A) = P(AB)/P(A)。AB是交集

- 分布：一个分布 描述了 一个随机变量所有可能的取值 以及 每个取值相应的概率。
  - 概率分布，所有可能发生事件的 可能性（即概率）的 集合，随机变量可能取到的值的概率规律。
  - 表示方法：一个二维坐标图，描述随机变量可能取到的每一个值（横轴） 和 这些值发生的概率大小（纵轴）
  - 一个分布 X~P 中，X 是随机变量集合（横轴、分布列），P是对应的每个事件发生的概率的集合（纵轴）
  - 分布的矩：描述一个or多个随机变量或者其概率分布的一种数值特征，用来描述数据集的形状。包括位置、散布、偏斜度等。eg：一阶原点矩 即为 该随机变量的均值。二阶中心矩 即为 该随机变量的方差。


- 数学期望 E(X) = ∑xi ×pi：所有随机变量X的 概率加权和。类似理解为“平均数”，∵已经乘了概率，∴可理解为加权平均。（离散型是求和∑，连续性是积分∫）
  - 期望的性质：Z = aX + b，则 E(Z) = aE(X) + b
  - eg：以离散型为例，一个射击选手一次射击，10环概率0.5，9环概率0.4，8环概率0.1。
  - 该射手一次射击最有可能设中的环数：E(X) = 10x0.5 + 9x0.4 + 8x0.1
  X|10 | 9 | 8 |
  -|---|---|---|
  P|0.5|0.4|0.1|

- 联合分布：两个或多个随机变量 同时发生的概率分布，即 同时 取各自的特定值 的概率分布情况，提供了多个随机变量共同变化的概率模型，展示了这些变量如何相互关联和影响。
  - 对于两个离散型随机变量 X 和 Y，它们的联合概率质量函数可以表示为：P(X=x_i ,Y=y_j)=p_ij，其中，p_ij是随机变量 X 取值为 x_i 且随机变量 Y 取值为 y_j 的概率。
  - 对于两个连续型随机变量 X 和 Y，它们的联合概率密度函数可以表示为：f(x,y)，描述了随机变量 X 和 Y 同时取值为 x 和 y 的概率密度。
  - 对于 相互独立的 多个随机变量，它们的联合分布可以表示为 各个随机变量分布的 乘积。——也叫“乘积分布 product distribution”。

- 边缘分布：从联合分布中推导出单个变量的分布。

- 条件分布：给定一个或多个变量的值时，其他变量的分布。eg：在已知Y=y的条件下，X的分布 可以表示为：P(X=x|Y=y)（离散型）或 f_X|Y(x|y)（连续型）

- 独立同分布：一组随机变量满足 ①任意两个变量之间 相互独立；②所有变量 服从 相同的概率分布。这组随机变量就是“独立同分布”的。
  - eg：多次抛硬币的结果（多个随机变量X1,X2,...,Xn）是独立同分布的，∵每次结果是独立的（正面or反面），每次结果的分布相同（正反概率都是0.5）


- 常见分布
  - 二项分布：随机变量X为 在固定次数的独立伯努利试验（Bernoulli trial）中成功的次数。每次伯努利试验只有两个可能的结果，通常称为“成功”和“失败”，即 1 和 0。
  - 均匀分布：随机变量X在某个区间内取值时，每个值出现的概率相等，P(X=xi)值相同。
  - 泊松分布：
  - 正态分布：

- 方差 D：

- 方差是减平均值，均方误差是减预测值


## 贝叶斯定理
- 衡量一个变量 在多大程度上 依赖另一个变量，即 有多受另一个变量的影响
- P(H|E) = [ P(H) ×P(E|H) ] / P(E)，贝叶斯定理，P(H|E)是后验概率，P(E)是？

- P(E|H)，P是似然概率，E是证据evidence，H是假设hypothesis？似然概率 = 在假设H成立时，证据E出现的概率？
（似-假设成立时，然-确定、证据？）

- P(H)，P是先验概率，在考虑新证据前，假设成立的可能性


- 大数定理：描述了在随机变量序列中，随着样本数量的增加，样本均值会趋近于总体均值的现象。大数定理有两种常见的形式：弱大数定理和强大数定理。


## 其他
- 排列
  - 从n个元素中选出r个，考虑选出的元素顺序时的所有可能情况
  - A = n! / (n-r)!
  - 注意选择并累乘。选出的第一个元素有n种可能性，第二个元素有n-1种可能，第三个有n-2种……第r个有n-(r-1)种，∴共有 n·(n-1)·(n-2)·...·(n-r+1) = n! / (n-r)!
  - eg：A52 = 5x4，A53 = 5x3x2

- 组合
  - 从n个元素中选出r个，不考虑选出的元素顺序时的所有可能情况
  - C = n! / r!(n-r)! = A / r!
  - 去掉顺序的影响。r个元素自己内部变序排列，共有 r! 种可能，∴排列去掉顺序之分就是组合数
  - eg：C52 = 5x4 / 2x1，C53 = C52


- 一个有n个元素的集合，共包含 2^n 个子集，包括全集和空集。每个元素有2种可能，选中or未选中。


# 线性 Linear——函数图像是一条【直线】

- 数学表达中，冒号 “：”通常表示 函数的定义，形如“函数名：输入→输出”。eg：f:x→y，即 y=f(x)

- 线性 包含于 非线性


[优化问题基础知识](https://blog.csdn.net/qq_42980908/article/details/137782925)

- 多项式：一元n次方程，只涉及 n、n^2、n的常数次方的函数

线性函数，线性模型或函数是指其输出与输入之间存在直接比例关系，即模型的响应与输入成正比。数学上，线性函数可以表示为f(x)=wx+b，其中 w 是权重（或斜率），b 是偏置（或截距），x 是输入变量。
多变量线性函数，

多变量线性函数（Multivariable Linear Function）是线性代数中的一个概念，它描述了一个或多个自变量（输入变量）与因变量（输出变量）之间的线性关系。在线性代数中，多变量线性函数通常具有以下形式：
f(x1,x2,...,xn) = a1 x1 + a2 x2 + ... + an xn + b
其中：
f 是因变量（函数值）。
x1, x2, ..., xn 是自变量（输入变量）。
a1, a2, ..., an 是对应于每个自变量的系数。
b 是函数的截距（当所有自变量为零时函数的值）。

特点：
直线性：在二维空间中，多变量线性函数的图像是一条直线。
平面性：在三维空间中，多变量线性函数的图像是一个平面。
超平面性：在更高维度的空间中，多变量线性函数的图像是一个超平面。
比例关系：函数的值与自变量之间存在直接的比例关系。


线性模型的例子包括线性回归、逻辑回归（在二分类问题中）以及线性判别分析。

# 非线性 Non-linear ——函数图像不是直线，如 可以是曲线等形状

二次函数：f(x)=x^2，描述抛物线运动或二次关系。
指数函数：f(x)=e^x，用于模拟人口增长或复利计算。
对数函数：f(x)=log(x)，用于对数或在信号处理中压缩数据。
神经网络：通过层叠层和激活函数（如ReLU）构建复杂的非线性模型。

非线性模型的例子包括多项式回归、神经网络、决策树、支持向量机（SVM）和核方法（如径向量核SVM）


- argXXX{f}：arg = argument 参数，意思是，找到 使函数f达到XXX效果的参数值、变量值
  - x∈agrmax{au + b}，x属于一个集合，这个集合的元素是，使au+b的值最大的u们
  - (x,y)∈agrmin{au + bv}，x、y属于一个集合，这个集合的元素是，使au+bv的值最大的u、v们


# 置信
- 置信区间：统计学中用于估计总体参数（如均值、比例等）的一个范围。它提供了一个区间估计，表示我们有多大的信心（置信水平）认为该区间包含了真实的总体参数。
- 置信度：
1. 统计学中的置信度
在统计学中，置信度通常与置信区间（Confidence Interval）相关联。置信区间是一个范围，用来估计某个参数的真实值。置信度表示这个区间包含真实参数值的概率。

- 例如，如果一个95%的置信区间为 [a, b]，这意味着我们有95%的置信度（信心）认为真实的参数值在这个区间内。这里的95%就是置信度。
- 95% 置信水平表示：如果重复抽样 100 次，构造 100 个置信区间，大约有 95 个区间会包含总体真实参数。（≠95%的概率）
  - 在统计学中，总体真实参数（True Population Parameter） 指的是研究者想要了解的、关于整个研究总体的固定但未知的数值特征。它是客观存在的真相，但由于无法普查整个总体，我们只能通过样本来推断它。

2. 机器学习中的置信度
在机器学习中，置信度通常用于衡量模型对某个预测结果的确定性。具体来说，置信度可以是一个概率值，表示模型认为某个预测结果是正确的概率。

常见的应用场景：
分类任务：在分类任务中，模型会输出每个类别的概率。最高的概率值可以被视为模型对该类别的置信度。例如，一个图像分类模型可能输出某个图像是猫的概率为0.9，这表示模型有90%的置信度认为这张图像是猫。
回归任务：在回归任务中，置信度可以表示模型对预测值的不确定性。例如，模型可能输出一个预测值及其对应的置信区间，表示模型对这个预测值的不确定程度。
对象检测：在对象检测任务中，模型会输出每个检测框的置信度，表示模型认为该检测框内确实存在目标物体的概率。
3. 其他领域的置信度
金融：在金融领域，置信度可以用来衡量投资决策的可靠性，例如某个投资策略的成功概率。
医学：在医学诊断中，置信度可以表示医生对某个诊断结果的信心程度。
自然语言处理：在自然语言处理中，置信度可以表示模型对某个句子或文本的理解程度。
总结
置信度是一个衡量确定性或可信度的概念，在不同的领域有不同的应用。在统计学中，它通常与置信区间相关联；在机器学习中，它用于衡量模型对预测结果的确定性。无论在哪个领域，置信度都是一个重要的指标，帮助我们理解和评估结果的可靠性。

- exp(x) = e^x，以e为底的指数函数

# 对偶
- 对偶：即 各种代换
- 通过对偶问题来求解原问题的最优解或者近似解，统称为原始-对偶算法
在数学和优化理论中，对偶性是指一个优化问题与其对应的另一个问题之间的关系。这种关系通常涉及原始问题（primal problem）和对偶问题（dual problem）之间的相互联系。

- 对偶性：
  - 对偶性是指在优化问题中，原始问题和对偶问题之间存在一种特殊的关系。通过对原始问题中的目标函数和约束条件进行变换，可以得到对偶问题，它是原始问题的一种对称形式。对偶性的概念在优化理论中非常重要，它帮助我们理解问题的不同方面，提供了一种从不同角度解决问题的方法。

- 对偶关系：
  - 对偶关系是指原始问题和对偶问题之间的关系。在优化理论中，原始问题和对偶问题之间存在一种对偶关系，即原始问题的最优解与对偶问题的最优解之间存在一种对应关系。这种对应关系通常表现为原始问题的最优值等于对偶问题的最优值，这被称为弱对偶性。如果原始问题和对偶问题同时满足一定条件，那么这种对应关系就被称为强对偶性。

- 对偶关系的重要性在于它提供了一种优化方法，即原始对偶方法。通过同时求解原始问题和对偶问题，可以得到原始问题和对偶问题的最优解，并且可以利用对偶问题的信息来加速原始问题的求解过程。对偶关系在优化领域和数值计算中有着广泛的应用，有助于提高优化问题的求解效率和精度。


# 导数 = 微商
- 函数上某一点的导数 = 该点的变化率 = 该点的斜率 = 以该点作切线
- 一阶导：f'(X0) = df(x0)/dx

- 求导和求微分的区别？


# 凸函数、凹函数
- 函数图像形状：凸函数 ∪、凹函数 ∩
- 凸函数 convex、凹函数 concave。（cave 洞穴，∩很形象）

- 概念来源：凸集
  - 凸集：集合C内任意两点间的线段也均在集合C内，则称集合C为凸集
  - 凸函数：定义在凸集上的函数

- 理解记忆：凸函数曲线上方的区域是凸的
  - 在一般的线性空间里都可以定义凸集（其中任意两点的连线都在集合内）,为了和这个定义统一，视曲线上方的区域为R2中的一个集合，凸函数的话，这个集合是凸集。
  - 以图像上方、y轴正方向 的 天空，为参考系，看形成的区域形状。在图像的上方画一条平行于x轴的直线，函数图像左右两端点以垂直于x轴的线段，连接上述直线，以形成一个封闭图形，图形是凹则曲线是凹函数。

ref：[为什么数学概念中，将凸起的函数称为凹函数？](https://www.zhihu.com/question/20014186)




- 凸函数：一阶导 递增↑，二阶导＞0
- 凹函数：一阶导 递减↓，二阶导＜0


[机器学习概念篇：一文详解凸函数和凸优化，干货满满](https://cloud.tencent.com/developer/news/335461)
[凸函数](https://mp.weixin.qq.com/s?__biz=MzIxODY0MjIxMQ==&mid=2247493868&idx=1&sn=915d644376bed85a1b476fb78ee0cd08&chksm=97e5caa0a09243b6adf14083fd024d9ba21b0844c8466d9e0035322b4f7cfa48f44fa5ced01e&scene=27)

