

G=(V,E), 一个匹配 M （M包含于E） 需满足 e1∩e2=∅，对于所有e1、e2∈M
最大匹配 |M|最多（|M|是这个匹配中的边的个数）
极大匹配

黄金标准

算法1.2 找一个极大匹配M，，算法1.2是 2-近似 的

（1）用同一个下界，能否得到更好的结果？——不能，算法的极限是2
（2）其他下界，能否更好？——不能，这个问题太难，100年内找不到更好的
（3）换一个算法，能否更好？——



- 确界原理：非空的有上界的集合必有上确界；非空的有下界的集合必有下确界。
  - 上确界：sup(X)，上界中最小的一个，集合X的上确界，supremum，取上限函数
  - 下确界：inf(X)，下界中最大的一个，集合X的下确界，infimum，取下限函数
  - eg：一个集合X可以有很多个上界、下界，(1,4) 的上界可以是4、5、6、7……下界可以是0、-1、-2……
  - eg：集合 M=[1,4]，sup(M)=4，inf(M)=1，max(M)=4，min(M)=1
  - eg：集合 N=(1,4)，sup(N)=4，inf(N)=1，max(N)=无，min(N)=无


- 动态规划，背包问题。动态规划的本质是遍历（初始化、迭代公式、最优解的选择）
- S(1,p)是解，A(1,p)是值，用值的存储，代替解的存储，省空间和计算量
Pmax = max profix(ai) ≤OPT≤n×Pmax



# 概率论

- 概率：
- 概率分布 X~P ：X是随机变量集合，P是对应的每个事件发生概率的集合？
- 数学期望 E(X)：随机变量的概率加权和？

- eg：一个射击选手一次射击，10环概率0.5，9环概率0.4，8环概率0.1。
- 该射手一次射击最有可能设中的环数：E(X) = 10x0.5 + 9x0.4 + 8x0.1
X|10 | 9 | 8 |
-|---|---|---|
P|0.5|0.4|0.1|

- 期望的性质：Z = aX + b，则 E(Z) = aE(X) + b


## 贝叶斯定理
- “|”= 在…的条件下。在“|”出现时，是在讨论总概率空间中的一个有限的部分

- P(H|E) = [ P(H) ×P(E|H) ] / P(E)，贝叶斯定理，P(H|E)是后验概率，P(E)是？

- P(E|H)，P是似然概率，E是证据evidence，H是假设hypothesis？似然概率 = 在假设H成立时，证据E出现的概率？
（似-假设成立时，然-确定、证据？）

- P(H)，P是先验概率，在考虑新证据前，假设成立的可能性





# 线性 Linear——函数图像是一条直线

- 数学表达中，“：”通常表示 函数的定义，形如“函数名：输入→输出”。eg：f:x→y，即 y=f(x)


[优化问题基础知识](https://blog.csdn.net/qq_42980908/article/details/137782925)

- 多项式：一元n次方程，只涉及 n、n^2、n的常数次方的函数

线性函数，线性模型或函数是指其输出与输入之间存在直接比例关系，即模型的响应与输入成正比。数学上，线性函数可以表示为f(x)=wx+b，其中 w 是权重（或斜率），b 是偏置（或截距），x 是输入变量。
多变量线性函数，

多变量线性函数（Multivariable Linear Function）是线性代数中的一个概念，它描述了一个或多个自变量（输入变量）与因变量（输出变量）之间的线性关系。在线性代数中，多变量线性函数通常具有以下形式：
f(x1,x2,...,xn) = a1 x1 + a2 x2 + ... + an xn + b
其中：
f 是因变量（函数值）。
x1, x2, ..., xn 是自变量（输入变量）。
a1, a2, ..., an 是对应于每个自变量的系数。
b 是函数的截距（当所有自变量为零时函数的值）。

特点：
直线性：在二维空间中，多变量线性函数的图像是一条直线。
平面性：在三维空间中，多变量线性函数的图像是一个平面。
超平面性：在更高维度的空间中，多变量线性函数的图像是一个超平面。
比例关系：函数的值与自变量之间存在直接的比例关系。


线性模型的例子包括线性回归、逻辑回归（在二分类问题中）以及线性判别分析。

# 非线性 Non-linear ——函数图像不是直线，如 可以是曲线等形状

二次函数：f(x)=x^2，描述抛物线运动或二次关系。
指数函数：f(x)=e^x，用于模拟人口增长或复利计算。
对数函数：f(x)=log(x)，用于对数或在信号处理中压缩数据。
神经网络：通过层叠层和激活函数（如ReLU）构建复杂的非线性模型。

非线性模型的例子包括多项式回归、神经网络、决策树、支持向量机（SVM）和核方法（如径向量核SVM）


- argXXX{f}：arg = argument 参数，意思是，找到 使函数f达到XXX效果的参数值、变量值
  - x∈agrmax{au + b}，x属于一个集合，这个集合的元素是，使au+b的值最大的u们
  - (x,y)∈agrmin{au + bv}，x、y属于一个集合，这个集合的元素是，使au+bv的值最大的u、v们


# 置信
- 置信区间：
- 置信度：
1. 统计学中的置信度
在统计学中，置信度通常与置信区间（Confidence Interval）相关联。置信区间是一个范围，用来估计某个参数的真实值。置信度表示这个区间包含真实参数值的概率。

例如，如果一个95%的置信区间为 [a, b]，这意味着我们有95%的置信度认为真实的参数值在这个区间内。这里的95%就是置信度。

2. 机器学习中的置信度
在机器学习中，置信度通常用于衡量模型对某个预测结果的确定性。具体来说，置信度可以是一个概率值，表示模型认为某个预测结果是正确的概率。

常见的应用场景：
分类任务：在分类任务中，模型会输出每个类别的概率。最高的概率值可以被视为模型对该类别的置信度。例如，一个图像分类模型可能输出某个图像是猫的概率为0.9，这表示模型有90%的置信度认为这张图像是猫。
回归任务：在回归任务中，置信度可以表示模型对预测值的不确定性。例如，模型可能输出一个预测值及其对应的置信区间，表示模型对这个预测值的不确定程度。
对象检测：在对象检测任务中，模型会输出每个检测框的置信度，表示模型认为该检测框内确实存在目标物体的概率。
3. 其他领域的置信度
金融：在金融领域，置信度可以用来衡量投资决策的可靠性，例如某个投资策略的成功概率。
医学：在医学诊断中，置信度可以表示医生对某个诊断结果的信心程度。
自然语言处理：在自然语言处理中，置信度可以表示模型对某个句子或文本的理解程度。
总结
置信度是一个衡量确定性或可信度的概念，在不同的领域有不同的应用。在统计学中，它通常与置信区间相关联；在机器学习中，它用于衡量模型对预测结果的确定性。无论在哪个领域，置信度都是一个重要的指标，帮助我们理解和评估结果的可靠性。

- exp(x) = e^x，以e为底的指数函数

# 对偶
- 对偶：即 各种代换
- 通过对偶问题来求解原问题的最优解或者近似解，统称为原始-对偶算法
在数学和优化理论中，对偶性是指一个优化问题与其对应的另一个问题之间的关系。这种关系通常涉及原始问题（primal problem）和对偶问题（dual problem）之间的相互联系。

- 对偶性：
  - 对偶性是指在优化问题中，原始问题和对偶问题之间存在一种特殊的关系。通过对原始问题中的目标函数和约束条件进行变换，可以得到对偶问题，它是原始问题的一种对称形式。对偶性的概念在优化理论中非常重要，它帮助我们理解问题的不同方面，提供了一种从不同角度解决问题的方法。

- 对偶关系：
  - 对偶关系是指原始问题和对偶问题之间的关系。在优化理论中，原始问题和对偶问题之间存在一种对偶关系，即原始问题的最优解与对偶问题的最优解之间存在一种对应关系。这种对应关系通常表现为原始问题的最优值等于对偶问题的最优值，这被称为弱对偶性。如果原始问题和对偶问题同时满足一定条件，那么这种对应关系就被称为强对偶性。

- 对偶关系的重要性在于它提供了一种优化方法，即原始对偶方法。通过同时求解原始问题和对偶问题，可以得到原始问题和对偶问题的最优解，并且可以利用对偶问题的信息来加速原始问题的求解过程。对偶关系在优化领域和数值计算中有着广泛的应用，有助于提高优化问题的求解效率和精度。


# 导数 = 微商
- 函数上某一点的导数 = 该点的变化率 = 该点的斜率 = 以该点作切线
- 一阶导：f'(X0) = df(x0)/dx

- 求导和求微分的区别？


# 凸函数、凹函数
- 函数图像形状：凸函数 ∪、凹函数 ∩
- 凸函数 convex、凹函数 concave。（cave 洞穴，∩很形象）

- 概念来源：凸集
  - 凸集：集合C内任意两点间的线段也均在集合C内，则称集合C为凸集
  - 凸函数：定义在凸集上的函数

- 理解记忆：凸函数曲线上方的区域是凸的
  - 在一般的线性空间里都可以定义凸集（其中任意两点的连线都在集合内）,为了和这个定义统一，视曲线上方的区域为R2中的一个集合，凸函数的话，这个集合是凸集。
  - 以图像上方、y轴正方向 的 天空，为参考系，看形成的区域形状。在图像的上方画一条平行于x轴的直线，函数图像左右两端点以垂直于x轴的线段，连接上述直线，以形成一个封闭图形，图形是凹则曲线是凹函数。

ref：[为什么数学概念中，将凸起的函数称为凹函数？](https://www.zhihu.com/question/20014186)




- 凸函数：一阶导 递增↑，二阶导＞0
- 凹函数：一阶导 递减↓，二阶导＜0


[机器学习概念篇：一文详解凸函数和凸优化，干货满满](https://cloud.tencent.com/developer/news/335461)
[凸函数](https://mp.weixin.qq.com/s?__biz=MzIxODY0MjIxMQ==&mid=2247493868&idx=1&sn=915d644376bed85a1b476fb78ee0cd08&chksm=97e5caa0a09243b6adf14083fd024d9ba21b0844c8466d9e0035322b4f7cfa48f44fa5ced01e&scene=27)

